### **项目陈述：基于RAG架构的智能PDF问答机器人（含独立Embedding服务）**

#### **1. 项目概述 (Project Overview)**

这是一个基于 **Next.js** 和 **TypeScript** 开发的全栈Web应用，实现了一个先进的 **检索增强生成 (RAG)** 智能问答机器人。项目的核心价值在于，它允许用户上传PDF格式的私有文档，并通过一个交互式的聊天界面，针对这些文档内容进行精准提问和获取回答。

与通用大语言模型不同，本项目的AI回答完全基于用户上传的文档内容，解决了大模型无法访问私有数据、可能产生“幻觉”以及数据隐私安全性等关键问题。

**项目亮点**：在开发过程中，我主导了一项关键的架构优化——**将Embedding（向量嵌入）功能从主应用中剥离，并独立部署为一个高性能的微服务**。这一举措不仅解耦了核心AI模块，还通过批量处理等优化手段，将文档处理的性能提升了**超过10倍**，显著改善了系统的响应速度和可扩展性。

#### **2. 核心功能 (Core Features)**

*   **文件上传与管理**:
    *   提供拖拽式文件上传界面，支持PDF文档。
    *   后端对上传文件进行唯一性标识，并将文件元数据存储在 **PostgreSQL** 数据库中。

*   **文档处理与向量化 (ETL Pipeline)**:
    *   **内容提取**: 使用 **LangChain** 的 `WebPDFLoader` 解析PDF文件，提取全文内容。
    *   **文本切分**: 采用 `RecursiveCharacterTextSplitter` 将长文本智能切分成更小的、有重叠的语义块（Chunks）。
    *   **向量嵌入 (调用独立服务)**: 主应用将切分好的文本块**批量发送**到我独立部署的 **Embedding微服务**。该服务负责将文本高效转换为高维向量。

*   **独立Embedding微服务 (关键贡献)**:
    *   **独立部署**: 将 `sentence-transformers/all-MiniLM-L6-v2` 模型封装成一个独立的Web服务，与主应用解耦。
    *   **高性能API**: 提供了 `/embed`（单个）和 `/embed/batch`（批量）两个API端点。
    *   **批量处理优化**: 实现并验证了批量处理接口，**处理效率比单个请求高10倍以上**，大幅降低了文档处理时的网络开销和总耗时。
    *   **职责单一**: 该服务专注于提供稳定、高效的文本到向量转换能力，便于未来独立升级和维护。

*   **智能问答 (RAG Workflow)**:
    *   **问题向量化**: 用户提问后，主应用调用**Embedding微服务**将问题文本转换为查询向量。
    *   **相似度检索**: 使用查询向量在 **Pinecone** 向量数据库中高效检索出最相关的文档上下文。
    *   **增强生成**: 将检索到的上下文注入到Prompt模板中，调用 **Cohere (Command R)** 大语言模型生成忠于原文的回答。
    *   **流式响应**: 将AI生成的回答以流式（Streaming）方式实时返回给前端，提升用户体验。

#### **3. 技术栈 (Tech Stack)**

*   **前端 (Frontend)**:
    *   **框架**: Next.js 15 (App Router)
    *   **语言**: TypeScript
    *   **UI组件**: shadcn/ui, Radix UI
    *   **样式**: Tailwind CSS
    *   **状态管理**: TanStack Query v5

*   **后端 (Backend)**:
    *   **框架**: Next.js API Routes (Serverless Functions)
    *   **数据库**: PostgreSQL
    *   **ORM**: Drizzle ORM
    *   **向量数据库**: Pinecone

*   **AI 与数据处理**:
    *   **LLM**: Cohere (Command R)
    *   **AI SDK**: Vercel AI SDK 3.0
    *   **文档加载/切分**: LangChain.js
    *   **Embedding服务**: **自研独立部署的微服务** (基于 `sentence-transformers` 模型)

#### **4. 项目架构 (Architecture)**

本项目采用现代化的**微服务**与**Serverless**混合架构，实现了高度的解耦和可扩展性：

1.  **展现层 (Presentation Layer)**: 由 **Next.js** 的React组件构成，负责UI渲染和用户交互。
2.  **主应用后端 (BFF - Backend for Frontend)**: 基于 **Next.js API Routes** 构建，负责处理业务逻辑，如用户请求路由、文件元数据管理、与Pinecone和LLM的通信等。它作为“指挥中心”，协调其他服务。
3.  **Embedding微服务 (Embedding Service)**:
    *   **独立进程**: 作为一个完全独立的服务运行，拥有自己的API (`https://embedding.badtom.dpdns.org`)。
    *   **核心职责**: 接收文本输入，利用加载在内存中的 `all-MiniLM-L6-v2` 模型进行计算，并返回向量。
    *   **优势**: 避免了主应用在Serverless环境下（冷启动）反复加载大模型的开销，并通过常驻内存和批量计算能力保证了高性能。
4.  **数据与AI服务层 (Data & AI Services)**:
    *   **Pinecone**: 外部向量数据库，用于存储和检索。
    *   **PostgreSQL**: 外部关系型数据库，用于持久化存储。
    *   **Cohere**: 外部大语言模型服务，用于生成最终回答。